{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send test prompts to multiple models for evaluation\n",
    "\n",
    "### Scenario\n",
    "You are choosing a large language model for your project.  You have narrowed down the list of potential models based on reading about the available models.  Now, you want to send test prompts to multiple \n",
    "\n",
    "Notebook sections:\n",
    "- [Step 1: Set up IBM watsonx.ai foundation model Python library prerequisites](#step1)\n",
    "- [Step 2: Create a function for prompting a model](#step2)\n",
    "- [Step 3: Create a simple function for prompting multiple models](#step3)\n",
    "- [Step 4: Create a function for sending multiple prompts to multiple models](#step4)\n",
    "- [Step 5: Create a function with model-specific prompt parameter overrides](#step5)\n",
    "- [Step 6: Create a function with model-specific prompt text overrides](#step6)\n",
    "\n",
    "By the end of this notebook, you'll have test results to help you decide which model to use for your project at this time:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/spackows/watsonx.ai-samples/main/sample-02_prompt-multiple-models/images/sample-02_prompt-multiple-models.png\" width=\"70%\" title=\"Image of DataFrame\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "## Step 1: Set up IBM watsonx.ai foundation model Python library prerequisites\n",
    "Before you can prompt a foundation model in watsonx.ai, you must perform the following setup tasks:\n",
    "- 1.1 Create an instance of the Watson Machine Learning service\n",
    "- 1.2 Associate the Watson Machine Learning instance with the current project\n",
    "- 1.3 Create an IBM Cloud API key\n",
    "- 1.4 Create a credentials dictionary for Watson Machine learning\n",
    "- 1.5 Look up the current project ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create an instance of the Watson Machine Learning service\n",
    "If you don't already have an instance of the IBM Watson Machine Learning service, you can create an instance of the service from the IBM Cloud catalog: <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\">Watson Machine Learning service</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Associate an instance of the Watson Machine Learning service with the current project\n",
    "The _current project_ is the project in which you are running this notebook.\n",
    "\n",
    "If an instance of Watson Machine Learning is not already associated with the current project, follow the instructions in this topic to do so: <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html?context=wx&audience=wdp\" target=\"_blank\">Adding associated services to a project</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create an IBM Cloud API key\n",
    "Create an IBM Cloud API key by following these instruction: <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui#create_user_key\" target=\"_blank\">Creating an IBM Cloud API key</a>\n",
    "\n",
    "Then paste your new IBM Cloud API key in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cloud_apikey = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create a credentials dictionary for Watson Machine learning\n",
    "See: [Authentication](https://ibm.github.io/watson-machine-learning-sdk/setup_cloud.html#authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_wml_credentials = { \n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\", \n",
    "    \"apikey\" : g_cloud_apikey\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Look up the current project ID\n",
    "The _current project_ is the project in which you are running this notebook.  You can get the ID of the current project programmatically by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "g_project_id = os.environ[\"PROJECT_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "## Step 2: Create a function for prompting a model\n",
    "\n",
    "See: [Foundation models Python library](https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "import json\n",
    "\n",
    "def generate( model_id, prompt_parameters, prompt_text, b_debug=False ):\n",
    "    model = Model( model_id, g_wml_credentials, prompt_parameters, g_project_id )\n",
    "    raw_response = model.generate( prompt_text )\n",
    "    if b_debug:\n",
    "        print( \"prompt_text:\\n'\" + prompt_text + \"'\\n\" )\n",
    "        print( \"raw_response:\\n\" + json.dumps( raw_response, indent=3 ) )\n",
    "    if ( \"results\" in raw_response ) \\\n",
    "       and ( len( raw_response[\"results\"] ) > 0 ) \\\n",
    "       and ( \"generated_text\" in raw_response[\"results\"][0] ):\n",
    "        return raw_response, raw_response[\"results\"][0][\"generated_text\"]\n",
    "    else:\n",
    "        return raw_response, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response, generated_output = generate( \"google/flan-t5-xxl\", {}, \"I took my dog for \", b_debug=True )\n",
    "\n",
    "print( \"\\ngenerated_output:\\n\" + generated_output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "## Step 3: Create a simple function for prompting multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def promptModels1( prompt_text, model_ids_arr, prompt_parameters={}, b_debug=False ):\n",
    "    all_results_arr = []\n",
    "    for model_id in model_ids_arr:\n",
    "        raw_response, generated_output = generate( model_id, prompt_parameters, prompt_text, b_debug )\n",
    "        generated_output = generated_output.strip()\n",
    "        if( \"system\" in raw_response ):\n",
    "            del( raw_response[\"system\"] )\n",
    "        all_results_arr.append( { \"model_id\"         : model_id, \n",
    "                                  \"model_short_id\"   : re.sub( r\"^.*\\/\", \"\", model_id ),\n",
    "                                  \"prompt_text\"      : prompt_text,\n",
    "                                  \"raw_response\"     : raw_response, \n",
    "                                  \"generated_output\" : generated_output } )\n",
    "    return all_results_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"I took my dog for a \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_parameters = {\n",
    "    \"decoding_method\" : \"greedy\",\n",
    "    \"min_new_tokens\"  : 0,\n",
    "    \"max_new_tokens\"  : 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Python library to list supported model IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "model_ids = list( map( lambda e: e.value, ModelTypes._member_map_.values() ) )\n",
    "model_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr = promptModels1( prompt_text, model_ids, prompt_parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( json.dumps( results_arr, indent=3 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function for displaying results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def styleDataFrame( styler ):\n",
    "    styler.set_properties( **{ \"text-align\" : \"left\", \"vertical-align\" : \"top\", \"padding\" : \"10px 20px 10px 20px\", \"font-size\" : \"120%\" } )\n",
    "    styler.set_table_styles( [ dict( selector=\"th\", props=\"text-align: center\" ) ] )\n",
    "    return styler\n",
    "\n",
    "def styleModelIDCol( styler ):\n",
    "    f_model_id = lambda v: \"width: 190px; min-width: 190px; max-width: 190px;\"\n",
    "    styler.applymap( f_model_id, subset=[ \"model_short_id\" ] )\n",
    "    return styler\n",
    "\n",
    "def resultsDF1( results_arr ):\n",
    "    df_org = pd.DataFrame( results_arr )\n",
    "    result_df = df_org[ [ \"model_short_id\", \"generated_output\" ] ]\n",
    "    result_df = result_df.sort_values( [ \"model_short_id\" ] ).reset_index( drop=True )\n",
    "    result_df = result_df.replace( { \"\\\\n\" : \"<br/>\" }, regex=True )\n",
    "    styler = result_df.style.pipe( styleDataFrame )\n",
    "    styler = styler.pipe( styleModelIDCol )\n",
    "    return styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resultsDF1( results_arr )\n",
    "\n",
    "print( \"Prompt text:\\n'\" + prompt_text + \"'\\n\" )\n",
    "print( \"Results:\" )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "## Step 4: Create a function for sending multiple prompts to multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptModels2( prompts_arr, model_ids_arr, prompt_parameters={}, b_debug=False ):\n",
    "    all_results_arr = []\n",
    "    for i in range( len( prompts_arr ) ):\n",
    "        prompt_text = prompts_arr[i]\n",
    "        for model_id in model_ids_arr:\n",
    "            raw_response, generated_output = generate( model_id, prompt_parameters, prompt_text, b_debug )\n",
    "            generated_output = generated_output.strip()\n",
    "            if( \"system\" in raw_response ):\n",
    "                del( raw_response[\"system\"] )\n",
    "            all_results_arr.append( { \"prompt_num\"       : i,\n",
    "                                      \"model_id\"         : model_id, \n",
    "                                      \"prompt_text\"      : prompt_text,\n",
    "                                      \"model_short_id\"   : re.sub( r\"^.*\\/\", \"\", model_id ),\n",
    "                                      \"raw_response\"     : raw_response, \n",
    "                                      \"generated_output\" : generated_output } )\n",
    "    return all_results_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_arr = [\n",
    "    \"I took my dog for a \",\n",
    "    \"I took my cat for a \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    \"bigscience/mt0-xxl\",\n",
    "    \"google/flan-t5-xxl\",\n",
    "    \"eleutherai/gpt-neox-20b\",\n",
    "    \"ibm/granite-13b-chat-v1\",\n",
    "    \"meta-llama/llama-2-13b-chat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr = promptModels2( prompts_arr, model_ids, prompt_parameters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a new display function that includes prompt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_prompt_col_width = \"200px\"\n",
    "\n",
    "def stylePromptCol( styler ):\n",
    "    f_prompt = lambda v: \"width: \"     + g_prompt_col_width + \"; \" + \\\n",
    "                         \"min-width: \" + g_prompt_col_width + \"; \" + \\\n",
    "                         \"max-width: \" + g_prompt_col_width + \";\"\n",
    "    styler.applymap( f_prompt, subset=[ \"prompt_text\" ] )\n",
    "    return styler\n",
    "\n",
    "def resultsDF2( results_arr ):\n",
    "    df_org = pd.DataFrame( results_arr )\n",
    "    result_df = df_org[ [ \"prompt_num\", \"model_short_id\", \"prompt_text\", \"generated_output\" ] ]\n",
    "    result_df = result_df.sort_values( [ \"prompt_num\", \"model_short_id\" ] ).reset_index( drop=True )\n",
    "    result_df = result_df.drop( \"prompt_num\", axis=1 )\n",
    "    result_df = result_df.replace( { \"\\\\n\" : \"<br/>\" }, regex=True )\n",
    "    styler = result_df.style.pipe( styleDataFrame )\n",
    "    styler = styler.pipe( styleModelIDCol )\n",
    "    styler = styler.pipe( stylePromptCol )\n",
    "    return styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF2( results_arr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step5\"></a>\n",
    "## Step 5: Create a function with model-specific parameter overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptModels3( prompts_arr, models_json, prompt_parameters={}, b_debug=False ):\n",
    "    all_results_arr = []\n",
    "    for i in range( len( prompts_arr ) ):\n",
    "        prompt_text = prompts_arr[i]\n",
    "        for model_id in models_json.keys():\n",
    "            model = models_json[ model_id ]\n",
    "            if( \"parameter_overrides\" in model ):\n",
    "                for parameter_name in model[\"parameter_overrides\"].keys():\n",
    "                    prompt_parameters[ parameter_name ] = model[\"parameter_overrides\"][ parameter_name ]\n",
    "            raw_response, generated_output = generate( model_id, prompt_parameters, prompt_text, b_debug )\n",
    "            generated_output = generated_output.strip()\n",
    "            if( \"system\" in raw_response ):\n",
    "                del( raw_response[\"system\"] )\n",
    "            all_results_arr.append( { \"prompt_num\"       : i,\n",
    "                                      \"model_id\"         : model_id, \n",
    "                                      \"model_short_id\"   : re.sub( r\"^.*\\/\", \"\", model_id ),\n",
    "                                      \"prompt_text\"      : prompt_text,\n",
    "                                      \"raw_response\"     : raw_response, \n",
    "                                      \"generated_output\" : generated_output } )\n",
    "    return all_results_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_json = {\n",
    "    \"bigscience/mt0-xxl\" : {},\n",
    "    \"google/flan-t5-xxl\" : {},\n",
    "    \"eleutherai/gpt-neox-20b\"     : { \"parameter_overrides\" : { \"stop_sequences\" : [ \"\\n\" ] } },\n",
    "    \"ibm/granite-13b-chat-v1\"     : { \"parameter_overrides\" : { \"max_new_tokens\" : 60 } },\n",
    "    \"meta-llama/llama-2-13b-chat\" : { \"parameter_overrides\" : { \"max_new_tokens\" : 80, \"stop_sequences\" : [ \"\\n\" ] } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr = promptModels3( prompts_arr, models_json, prompt_parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF2( results_arr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step6\"></a>\n",
    "## Step 6: Create a function with model-specific prompt overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptModels4( prompts_arr, models_json, prompt_parameters={}, b_debug=False ):\n",
    "    all_results_arr = []\n",
    "    for i in range( len( prompts_arr ) ):\n",
    "        prompt_text_org = prompts_arr[i]\n",
    "        for model_id in models_json.keys():\n",
    "            model = models_json[ model_id ]\n",
    "            if( \"parameter_overrides\" in model ):\n",
    "                for parameter_name in model[\"parameter_overrides\"].keys():\n",
    "                    prompt_parameters[ parameter_name ] = model[\"parameter_overrides\"][ parameter_name ]\n",
    "            prompt_text = model[\"prompt_template\"] % ( prompt_text_org ) if ( \"prompt_template\" in model ) else prompt_text_org\n",
    "            raw_response, generated_output = generate( model_id, prompt_parameters, prompt_text, b_debug )\n",
    "            generated_output = generated_output.strip()\n",
    "            if( \"system\" in raw_response ):\n",
    "                del( raw_response[\"system\"] )\n",
    "            all_results_arr.append( { \"prompt_num\"       : i,\n",
    "                                      \"model_id\"         : model_id, \n",
    "                                      \"model_short_id\"   : re.sub( r\"^.*\\/\", \"\", model_id ),\n",
    "                                      \"prompt_text\"      : prompt_text,\n",
    "                                      \"raw_response\"     : raw_response, \n",
    "                                      \"generated_output\" : generated_output } )\n",
    "    return all_results_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptneox_template = \"\"\"I took my bird for a flight around the yard, it said *tweet*.\n",
    "I took my horse for a ride along the trail, it said *snort*.\n",
    "I took my fish for a swim in the lake, it said *bubbles*.\n",
    "I took my mouse for a cycle around the block, it said *squeak*.\n",
    "I took my cow for a drive to the mountains, it said *moo*.\n",
    "I took my donkey for a trek up the hill, it said *heehaw*.\n",
    "%s\"\"\"\n",
    "\n",
    "\n",
    "granite_template = \"\"\"As a helpful assistant, complete the sentence started by the human.\n",
    "\n",
    "Human: %s\n",
    "Assistant: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "models_json = {\n",
    "    \"bigscience/mt0-xxl\" : {},\n",
    "    \"google/flan-t5-xxl\" : {},\n",
    "    \"eleutherai/gpt-neox-20b\"     : { \"parameter_overrides\" : { \"stop_sequences\" : [ \"\\n\" ] },\n",
    "                                      \"prompt_template\"     : gptneox_template },\n",
    "    \"ibm/granite-13b-chat-v1\"     : { \"parameter_overrides\" : { \"max_new_tokens\" : 60 },\n",
    "                                      \"prompt_template\"     : granite_template },\n",
    "    \"meta-llama/llama-2-13b-chat\" : { \"parameter_overrides\" : { \"max_new_tokens\" : 80, \"stop_sequences\" : [ \"\\n\" ] } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr = promptModels4( prompts_arr, models_json, prompt_parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_prompt_col_width = \"440px\"\n",
    "\n",
    "resultsDF2( results_arr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new display function to view generated output only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def styleOutputCol( styler, col_names_arr ):\n",
    "    f_prompt = lambda v: \"width: 350px; \" + \\\n",
    "                         \"min-width: 350px; \" + \\\n",
    "                         \"max-width: 350px;\"\n",
    "    styler.applymap( f_prompt, subset=col_names_arr )\n",
    "    return styler\n",
    "\n",
    "\n",
    "def resultsDF3( results_arr ):\n",
    "    \n",
    "    df_org = pd.DataFrame( results_arr )\n",
    "    result_df = df_org[ [ \"prompt_num\", \"model_short_id\", \"generated_output\" ] ]\n",
    "    result_df = result_df.sort_values( [ \"prompt_num\", \"model_short_id\" ] )\n",
    "    result_df = result_df.reset_index( drop=True )\n",
    "    result_df = result_df.replace( { \"\\\\n\" : \"<br/>\" }, regex=True )\n",
    "    \n",
    "    new_df = pd.DataFrame( columns=[ \"model_short_id\" ] )\n",
    "    prompt_nums_arr = sorted( result_df[ \"prompt_num\" ].unique() )\n",
    "    col_names_arr = []\n",
    "    for prompt_num in prompt_nums_arr:\n",
    "        col_name = \"output \" + str( prompt_num )\n",
    "        col_names_arr.append( col_name)\n",
    "        df_tmp = result_df[ result_df[\"prompt_num\"] == prompt_num ]\n",
    "        df_tmp = df_tmp.drop( \"prompt_num\", axis=1 )\n",
    "        df_tmp = df_tmp.rename( columns={ \"generated_output\": col_name } )\n",
    "        new_df = new_df.merge( df_tmp, how=\"right\", on=\"model_short_id\" )\n",
    "    \n",
    "    styler = new_df.style.pipe( styleDataFrame )\n",
    "    styler = styler.pipe( styleModelIDCol )\n",
    "    styler = styler.pipe( styleOutputCol, col_names_arr )\n",
    "        \n",
    "    return styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF3( results_arr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
